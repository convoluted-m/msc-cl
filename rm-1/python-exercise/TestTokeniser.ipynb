{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd728786-a16e-410d-b317-05c999f5c161",
   "metadata": {
    "id": "dd728786-a16e-410d-b317-05c999f5c161"
   },
   "source": [
    "# Python-programming task\n",
    "\n",
    "In this task, you will be asked to implement a tokeniser and a function that provides simple corpus statistics.\n",
    "\n",
    "In a separate text file called tokeniser.py, you should define a `Tokeniser` class with the following public methods:\n",
    "\n",
    "```python\n",
    "class Tokeniser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def tokenise_on_punctuation(self, text):\n",
    "        pass\n",
    "\n",
    "    def train(self, text):\n",
    "        pass\n",
    "\n",
    "    def tokenise(self, text, use_unk=False):\n",
    "        pass\n",
    "\n",
    "    def tokenise_with_count_threshold(self, text, threshold, use_unk=False):\n",
    "        pass\n",
    "\n",
    "    def tokenise_with_freq_threshold(self, text, threshold, use_unk=False):\n",
    "        pass\n",
    "```\n",
    "\n",
    "The `__init__` method should initialise the tokeniser. The `tokenise_on_punctuation` method can be used without training the tokeniser. It should split the input on any kind of whitespace (including tab characters and newlines) and on punctuation signs. The punctuation signs are as follows: `!\"#%&'()*,-./:;?@[\\]_{}¡§«¶·»¿‘’“”–—`\n",
    "\n",
    "The `train` method should take as input a corpus and prepare the tokeniser for use on new texts. The trained tokeniser can be used by invoking the last three methods. If the user tries to invoke them _before_ training the tokeniser, your code should raise a `RuntimeError` with the message `The tokeniser has not been trained yet.`.\n",
    "\n",
    "The methods should work as follows:\n",
    "\n",
    "1. `tokenise` splits the input on punctuation and whitespace and then goes over tokens. If a token is found in the vocabulary, it is added to the output. Otherwise the behaviour is determined by the `use_unk` flag: if it is set to `True`, unknown tokens should be replaced with 'UNK'; otherwise they should be split into individual characters.\n",
    "2. `tokenise_with_count_threshold` first also splits the input on whitespace and vocabulary. It then checks how many times a token appeared in the training corpus. If it appeared `threshold` times or more, it is added to the output; otherwise it is treated as an unknown token in the same way as above.\n",
    "3. `tokenise_with_freq_threshold` does the same thing with regard to _relative frequency_ of the current token in the training corpus. Relative frequency ranges from 0 (not found in the training corpus) to 1 (all of the tokens from the training corpus are this same token).\n",
    "\n",
    "All the operations of the tokeniser should be case dependent, i.e. 'a' and 'A' are different tokens, same as 'this' and 'tHis'.\n",
    "\n",
    "You can use the code below to test the behaviour of your tokeniser. When evaluating your code, we will use the same code but with other inputs. You are also encouraged to use other inputs to make sure that the code is doing what it is supposed to do.\n",
    "\n",
    "You can implement as many additional methods of the `Tokeniser` class, auxiliary functions, or even auxiliary classes as you want. The only restriction is that you can only use the Python standard library and that all your code resides in the `tokeniser.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fb294-e4ca-4e31-90f0-f6f3cd3d0c84",
   "metadata": {
    "id": "5d0fb294-e4ca-4e31-90f0-f6f3cd3d0c84"
   },
   "source": [
    "In addition to the tokeniser, in the same file you should also implement a function called `get_stats`. It should take a tokenised corpus as input and return a dictionary with the following fields:\n",
    "\n",
    "1. `type_count`, the number of different tokens in the corpus\n",
    "2. `token_count`, the total number of tokens in the corpus\n",
    "3. `type_token_ratio`, the ratio between the two, a common measure of lexical variability\n",
    "4. `token_count_by_length`, the number of tokens of different lengths, measured in characters, found in the corpus\n",
    "5. `average_token_length`, the mean length in characters of all tokens in the corpus\n",
    "6. `token_length_std`, the standard average of the same\n",
    "\n",
    "See the end of this notebook for a usage example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459a55dd-62fe-453e-80c6-b9526e139ee3",
   "metadata": {
    "id": "459a55dd-62fe-453e-80c6-b9526e139ee3"
   },
   "outputs": [],
   "source": [
    "from tokeniser import Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c4601a-8557-4a90-8e2d-bab7c46c951d",
   "metadata": {
    "id": "97c4601a-8557-4a90-8e2d-bab7c46c951d"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "raw_bytes = urllib.request.urlopen(\n",
    "    'http://www.sls.hawaii.edu/bley-vroman/brown.txt')\n",
    "brown_corpus = raw_bytes.read().decode('utf8').replace('\\r\\n', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fc9c62-7a89-4e0e-8d29-6235544b0ad4",
   "metadata": {
    "id": "a0fc9c62-7a89-4e0e-8d29-6235544b0ad4"
   },
   "outputs": [],
   "source": [
    "brown_250 = brown_corpus[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6576de34-3057-48b3-861d-01d8ff1bc3aa",
   "metadata": {
    "id": "6576de34-3057-48b3-861d-01d8ff1bc3aa"
   },
   "outputs": [],
   "source": [
    "tokeniser = Tokeniser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c19578-2f73-4bb7-b3c5-01cc52ab5101",
   "metadata": {
    "id": "34c19578-2f73-4bb7-b3c5-01cc52ab5101",
    "outputId": "923b43e1-7354-437b-d43d-3044a4a8423a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', 's', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place', 'The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', 'which', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    tokeniser.tokenise_on_punctuation(brown_250)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a1e828-aac6-4d20-aef0-900c7ad82364",
   "metadata": {
    "id": "58a1e828-aac6-4d20-aef0-900c7ad82364",
    "outputId": "8a5ec6b8-bf63-433e-8045-0dab684753b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Budget',\n",
       " 'l',\n",
       " 'effort',\n",
       " 'demandé',\n",
       " 'aux',\n",
       " 'départements',\n",
       " 'sera',\n",
       " 'réduit',\n",
       " 'très',\n",
       " 'significativement',\n",
       " 'annonce',\n",
       " 'Michel',\n",
       " 'Barnier']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.tokenise_on_punctuation(\n",
    "    'Budget : l’effort demandé aux départements sera réduit «très significativement»,'\n",
    "    ' annonce Michel Barnier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7109673-1a0c-4e2a-afef-61347ae756cf",
   "metadata": {
    "id": "e7109673-1a0c-4e2a-afef-61347ae756cf",
    "outputId": "7db3edc9-dfe0-4550-c2e0-72a963234bff"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The tokeniser has not been trained yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Should fail at this time\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokeniser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrown_250\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/source/msc-cl/research-methods-1/python-exercise/tokeniser.py:39\u001b[0m, in \u001b[0;36mTokeniser.tokenise\u001b[0;34m(self, text, use_unk)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenise\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, use_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# should fail when  invoked without training\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_trained: \n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tokeniser has not been trained yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         output \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# create an empty list to store the output - vocab of tokens\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The tokeniser has not been trained yet."
     ]
    }
   ],
   "source": [
    "# Should fail at this time\n",
    "tokeniser.tokenise(brown_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90478427-13a6-44d8-86a1-b236051ecb15",
   "metadata": {
    "id": "90478427-13a6-44d8-86a1-b236051ecb15"
   },
   "outputs": [],
   "source": [
    "tokeniser.train(brown_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5e9ed4-3647-44b9-b896-b7db1ad6709a",
   "metadata": {
    "id": "7a5e9ed4-3647-44b9-b896-b7db1ad6709a",
    "outputId": "ad377d32-2b3c-42c3-ccae-1dab9e05fbed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', 's', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place', 'The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', 'which', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(repr(tokeniser.tokenise(brown_250)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d55890-48eb-44de-92f9-7eb111604bbe",
   "metadata": {
    "id": "15d55890-48eb-44de-92f9-7eb111604bbe",
    "outputId": "2667f697-9251-4c42-de62-8492e3edfb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Budget', 'l', 'effort', 'd', 'e', 'm', 'a', 'n', 'd', 'é', 'aux', 'd', 'é', 'p', 'a', 'r', 't', 'e', 'm', 'e', 'n', 't', 's', 'sera', 'r', 'é', 'd', 'u', 'i', 't', 't', 'r', 'è', 's', 's', 'i', 'g', 'n', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'v', 'e', 'm', 'e', 'n', 't', 'a', 'n', 'n', 'o', 'n', 'c', 'e', 'M', 'i', 'c', 'h', 'e', 'l', 'B', 'a', 'r', 'n', 'i', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "print(repr(tokeniser.tokenise('Budget : l’effort demandé aux départements sera réduit «très significativement»,'\n",
    "                              ' annonce Michel Barnier')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a65f709b-e859-43e3-9071-1925ccef0cca",
   "metadata": {
    "id": "a65f709b-e859-43e3-9071-1925ccef0cca",
    "outputId": "e268f2fc-fb78-4e89-e226-ef5de7018f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Budget', 'l', 'effort', 'UNK', 'aux', 'UNK', 'sera', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    tokeniser.tokenise(\n",
    "    'Budget : l’effort demandé aux départements sera réduit «très significativement», annonce Michel Barnier',\n",
    "    use_unk=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ce109c-eb03-40c2-af77-8376d71a4743",
   "metadata": {
    "id": "47ce109c-eb03-40c2-af77-8376d71a4743",
    "outputId": "5ad421ca-68e6-46d7-9478-51435d22a2cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'F', 'u', 'l', 't', 'o', 'n', 'C', 'o', 'u', 'n', 't', 'y', 'G', 'r', 'a', 'n', 'd', 'J', 'u', 'r', 'y', 'said', 'F', 'r', 'i', 'd', 'a', 'y', 'an', 'i', 'n', 'v', 'e', 's', 't', 'i', 'g', 'a', 't', 'i', 'o', 'n', 'of', 'A', 't', 'l', 'a', 'n', 't', 'a', 's', 'r', 'e', 'c', 'e', 'n', 't', 'p', 'r', 'i', 'm', 'a', 'r', 'y', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', 'p', 'r', 'o', 'd', 'u', 'c', 'e', 'd', 'no', 'e', 'v', 'i', 'd', 'e', 'n', 'c', 'e', 'that', 'any', 'i', 'r', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 't', 'i', 'e', 's', 't', 'o', 'o', 'k', 'p', 'l', 'a', 'c', 'e', 'The', 'j', 'u', 'r', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'said', 'in', 't', 'e', 'r', 'm', 'e', 'n', 'd', 'p', 'r', 'e', 's', 'e', 'n', 't', 'm', 'e', 'n', 't', 's', 'that', 'the', 'C', 'i', 't', 'y', 'E', 'x', 'e', 'c', 'u', 't', 'i', 'v', 'e', 'C', 'o', 'm', 'm', 'i', 't', 't', 'e', 'e', 'which', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    tokeniser.tokenise_with_count_threshold(brown_250, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f07e68d-1c36-48df-9bd4-151ff04ea93c",
   "metadata": {
    "id": "1f07e68d-1c36-48df-9bd4-151ff04ea93c",
    "outputId": "b93ef521-74e4-4b52-d343-d28f54fe2982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'UNK', 'UNK', 'UNK', 'UNK', 'said', 'UNK', 'an', 'UNK', 'of', 'UNK', 's', 'UNK', 'UNK', 'UNK', 'UNK', 'no', 'UNK', 'that', 'any', 'UNK', 'UNK', 'UNK', 'The', 'UNK', 'UNK', 'said', 'in', 'UNK', 'UNK', 'UNK', 'that', 'the', 'UNK', 'UNK', 'UNK', 'which', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    tokeniser.tokenise_with_count_threshold(brown_250, 1000, use_unk=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcbde8a5-06d5-456e-968f-71db5908f6fc",
   "metadata": {
    "id": "fcbde8a5-06d5-456e-968f-71db5908f6fc",
    "outputId": "2b591346-9bf9-449d-e042-eca0958258fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'J', 'u', 'r', 'y', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', 's', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'i', 'r', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'i', 't', 'i', 'e', 's', 'took', 'place', 'The', 'jury', 'further', 'said', 'in', 'term', 'end', 'p', 'r', 'e', 's', 'e', 'n', 't', 'm', 'e', 'n', 't', 's', 'that', 'the', 'City', 'E', 'x', 'e', 'c', 'u', 't', 'i', 'v', 'e', 'Committee', 'which', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    tokeniser.tokenise_with_freq_threshold(brown_250, 0.00001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "145440d9-5cb5-451e-b943-4187e77292da",
   "metadata": {
    "id": "145440d9-5cb5-451e-b943-4187e77292da",
    "outputId": "37386b02-f08d-4300-a188-30d808085d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'UNK', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', 's', 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'UNK', 'took', 'place', 'The', 'jury', 'further', 'said', 'in', 'term', 'end', 'UNK', 'that', 'the', 'City', 'UNK', 'Committee', 'which', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    tokeniser.tokenise_with_freq_threshold(brown_250, 0.00001, True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db9758c-089f-4cf2-81fa-35c21198515d",
   "metadata": {
    "id": "4db9758c-089f-4cf2-81fa-35c21198515d"
   },
   "outputs": [],
   "source": [
    "raw_bytes_austen = urllib.request.urlopen(\n",
    "    'https://www.gutenberg.org/cache/epub/1342/pg1342.txt')\n",
    "pride_and_prejudice = raw_bytes_austen.read().decode('utf8').replace('\\r\\n', '\\n').replace('\\ufeff', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87be760e-0d66-4bcb-aa64-15bcdf4731b2",
   "metadata": {
    "id": "87be760e-0d66-4bcb-aa64-15bcdf4731b2"
   },
   "outputs": [],
   "source": [
    "pride_and_prejudice_tokenised = tokeniser.tokenise_with_count_threshold(pride_and_prejudice, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6890ed0-a32b-4829-a51c-e6c128e1af63",
   "metadata": {
    "id": "e6890ed0-a32b-4829-a51c-e6c128e1af63",
    "outputId": "54b587c7-147a-43bc-a530-866048ea5f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'P', 'r', 'o', 'j', 'e', 'c', 't', 'of', 'P', 'r', 'i', 'd', 'e', 'and', 'P', 'r', 'e', 'j', 'u', 'd', 'i', 'c', 'e', 'This', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'P', 'r', 'o', 'j', 'e', 'c', 't', 'included', 'with', 'this', 'or', 'at', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'Title', 'P', 'r', 'i', 'd', 'e', 'and', 'P', 'r', 'e', 'j', 'u', 'd', 'i', 'c', 'e', 'A', 'u', 't', 'h', 'o', 'r', 'Jane', 'date', 'June', '1', 'Most', 'recently', 'u', 'p', 'd', 'a', 't', 'e', 'd', 'October', '29', 'L', 'a', 'n', 'g', 'u', 'a', 'g']\n"
     ]
    }
   ],
   "source": [
    "print(repr(\n",
    "    pride_and_prejudice_tokenised[:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6545eeba-5875-4d99-9ced-2a2f389e3161",
   "metadata": {
    "id": "6545eeba-5875-4d99-9ced-2a2f389e3161"
   },
   "outputs": [],
   "source": [
    "from tokeniser import get_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecc63201-0ade-4818-b1a0-84b05273074b",
   "metadata": {
    "id": "ecc63201-0ade-4818-b1a0-84b05273074b",
    "outputId": "70bb84a0-59c6-4a52-9758-2003ad3a93c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type_count: 4831\n",
      "token_count: 159313\n",
      "type_token_ratio: 0.030323953475234287\n",
      "token_count_by_length: {3: 30186, 1: 41196, 2: 24992, 4: 23042, 6: 8719, 8: 4286, 5: 11450, 12: 738, 10: 1818, 7: 7568, 9: 4126, 13: 197, 11: 915, 14: 58, 16: 1, 15: 21}\n",
      "average_token_length: 3.4439938988029852\n",
      "token_length_std: 2.8867522081277435\n"
     ]
    }
   ],
   "source": [
    "get_stats(pride_and_prejudice_tokenised)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "msc-cl-mk6x0iyF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
